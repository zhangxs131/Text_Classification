{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "run_glue_for_thucnews.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMM0UAuy+525Jc/aBYVztGm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhangxs131/Text_Classification/blob/main/run_glue_for_thucnews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##使用run_glue.py进行THUCNEWS的标题文本分类\n"
      ],
      "metadata": {
        "id": "Fu1VPKq_Y1-9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Imjg4YKRWSq"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZOf7DCGT2tD",
        "outputId": "1f39566b-66b3-495f-82e0-a85281b5810b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 96131, done.\u001b[K\n",
            "remote: Counting objects: 100% (209/209), done.\u001b[K\n",
            "remote: Compressing objects: 100% (110/110), done.\u001b[K\n",
            "remote: Total 96131 (delta 117), reused 154 (delta 84), pack-reused 95922\u001b[K\n",
            "Receiving objects: 100% (96131/96131), 88.98 MiB | 14.65 MiB/s, done.\n",
            "Resolving deltas: 100% (70646/70646), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "这里需要把transformers/examples/pytorch/text-classification/run_glue.py 文件进行一点修改\n",
        "\n",
        "就是transfoerms版本检测，一般在50行左右，因为原来代码版本为测试版4.19.0dev0，我们下载是正式版4.18，不修改会报错。"
      ],
      "metadata": {
        "id": "FB-E-i4jZP8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#下载数据\n",
        "from datasets import load_dataset\n",
        "\n",
        "cnews=load_dataset('seamew/THUCNewsTitle')\n",
        "\n",
        "cnews['train'].to_csv('train.csv',index=False)\n",
        "cnews['validation'].to_csv('valid.csv',index=False)\n",
        "cnews['test'].to_csv('test.csv',index=False)"
      ],
      "metadata": {
        "id": "vEOWq8whZAKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#先用个bert-base-chinese试试水\n",
        "!python transformers/examples/pytorch/text-classification/run_glue.py \\\n",
        "  --model_name_or_path bert-base-chinese \\\n",
        "  --train_file train.csv \\\n",
        "  --validation_file valid.csv\\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 16 \\\n",
        "  --fp16 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --save_strategy epoch \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --output_dir /tmp/classifier/"
      ],
      "metadata": {
        "id": "WWn1Z_WtT9DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##bert-base-chinese 跑了3轮，得到96的acc，效果确实已经很不错了"
      ],
      "metadata": {
        "id": "a_UTfsM6qgjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#再试试bert-wwm-ext 哈工大hfl的那个模型\n",
        "!python transformers/examples/pytorch/text-classification/run_glue.py \\\n",
        "  --model_name_or_path hfl/chinese-bert-wwm-ext \\\n",
        "  --train_file train.csv \\\n",
        "  --validation_file valid.csv\\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 16 \\\n",
        "  --fp16 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --save_strategy epoch \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --output_dir /tmp/bert-wwm-ext/"
      ],
      "metadata": {
        "id": "fyPi-DF6qfmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#使用bert-wwm-ext准确率比上面高了4个千分点，96.4左右acc，还算可以吧"
      ],
      "metadata": {
        "id": "rSemunirx9vJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rjieba"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSP0dPYdYkF_",
        "outputId": "2c33862a-be6a-457a-827d-bbf9c65687c2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rjieba\n",
            "  Downloading rjieba-0.1.11-cp36-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 28.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: rjieba\n",
            "Successfully installed rjieba-0.1.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#再试试woberta，苏神他们公司的，但hf上下载太少，感觉可能是假的\n",
        "!python transformers/examples/pytorch/text-classification/run_glue.py \\\n",
        "  --model_name_or_path junnyu/wobert_chinese_plus_base \\\n",
        "  --train_file train.csv \\\n",
        "  --validation_file valid.csv\\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 16 \\\n",
        "  --fp16 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --save_strategy epoch \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --output_dir /content/woberta/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnquBem9Xlqp",
        "outputId": "0c5ea84e-2602-4d6c-db28-a9be080e17a6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05/21/2022 02:11:50 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "05/21/2022 02:11:50 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/woberta/runs/May21_02-11-49_3f99443cd0d9,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=/content/woberta/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/woberta/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "05/21/2022 02:11:50 - INFO - __main__ - load a local file for train: train.csv\n",
            "05/21/2022 02:11:50 - INFO - __main__ - load a local file for validation: valid.csv\n",
            "05/21/2022 02:11:50 - WARNING - datasets.builder - Using custom data configuration default-2b21be7530cbf85e\n",
            "05/21/2022 02:11:50 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "05/21/2022 02:11:50 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-2b21be7530cbf85e/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\n",
            "05/21/2022 02:11:50 - WARNING - datasets.builder - Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-2b21be7530cbf85e/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n",
            "05/21/2022 02:11:50 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-2b21be7530cbf85e/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\n",
            "100% 2/2 [00:00<00:00, 322.83it/s]\n",
            "[INFO|configuration_utils.py:659] 2022-05-21 02:11:51,899 >> loading configuration file https://huggingface.co/junnyu/wobert_chinese_plus_base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c97d1d931723e64159a8452ca783084fba677841ce8f7a0d8c18f52602eecce.bb7c9f81c5e5900fece4ad4eba2e8ea53a232903011dd0002de2dff863c76386\n",
            "[INFO|configuration_utils.py:708] 2022-05-21 02:11:51,900 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"junnyu/wobert_chinese_plus_base\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\",\n",
            "    \"9\": \"LABEL_9\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8,\n",
            "    \"LABEL_9\": 9\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-05-21 02:11:58,333 >> loading file https://huggingface.co/junnyu/wobert_chinese_plus_base/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/b1fdd2f1205ab6bdc8709b9579a2266b940878c0b0176483ed7228b5456d69f3.4bc5930c7f05c3b0a7ed8fd96e09552561978f34a1322701fad7bd05e925918a\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-05-21 02:11:58,333 >> loading file https://huggingface.co/junnyu/wobert_chinese_plus_base/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-05-21 02:11:58,333 >> loading file https://huggingface.co/junnyu/wobert_chinese_plus_base/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-05-21 02:11:58,333 >> loading file https://huggingface.co/junnyu/wobert_chinese_plus_base/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/a262a4ed1cada54b2b5133f866d4c8c4256bc3a1601b8e9086eac2fd7656f0a2.f982506b52498d4adb4bd491f593dc92b2ef6be61bfdbe9d30f53f963f9f5b66\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-05-21 02:11:58,333 >> loading file https://huggingface.co/junnyu/wobert_chinese_plus_base/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/0c74af14ea1253a34227009a752bf680e04f6198c3df5b369747396d29f9c966.1be22fc1c17455d51cce3d49d4fa0dd47549c662a5fc235f0d190095dc90c78d\n",
            "[INFO|hub.py:583] 2022-05-21 02:11:59,521 >> https://huggingface.co/junnyu/wobert_chinese_plus_base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpl9v827iw\n",
            "Downloading: 100% 477M/477M [00:24<00:00, 20.3MB/s]\n",
            "[INFO|hub.py:587] 2022-05-21 02:12:25,226 >> storing https://huggingface.co/junnyu/wobert_chinese_plus_base/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/869efc2e3460ac7ff602f49efe3528dc576d7bad30009ed763a4cdb801dd946d.de6c26adc59bb0e8b800d8c8ee8d88dffa21c041fd5f8eba52a36dbb398c851e\n",
            "[INFO|hub.py:595] 2022-05-21 02:12:25,227 >> creating metadata file for /root/.cache/huggingface/transformers/869efc2e3460ac7ff602f49efe3528dc576d7bad30009ed763a4cdb801dd946d.de6c26adc59bb0e8b800d8c8ee8d88dffa21c041fd5f8eba52a36dbb398c851e\n",
            "[INFO|modeling_utils.py:1953] 2022-05-21 02:12:25,227 >> loading weights file https://huggingface.co/junnyu/wobert_chinese_plus_base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/869efc2e3460ac7ff602f49efe3528dc576d7bad30009ed763a4cdb801dd946d.de6c26adc59bb0e8b800d8c8ee8d88dffa21c041fd5f8eba52a36dbb398c851e\n",
            "[WARNING|modeling_utils.py:2255] 2022-05-21 02:12:26,837 >> Some weights of the model checkpoint at junnyu/wobert_chinese_plus_base were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2266] 2022-05-21 02:12:26,837 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at junnyu/wobert_chinese_plus_base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/50 [00:00<?, ?ba/s]05/21/2022 02:12:26 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-2b21be7530cbf85e/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-72d06c97fe5f2aa0.arrow\n",
            "Running tokenizer on dataset: 100% 50/50 [00:03<00:00, 12.72ba/s]\n",
            "Running tokenizer on dataset:   0% 0/5 [00:00<?, ?ba/s]05/21/2022 02:12:30 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-2b21be7530cbf85e/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-d8b03f6f8bc78285.arrow\n",
            "Running tokenizer on dataset: 100% 5/5 [00:00<00:00, 14.48ba/s]\n",
            "05/21/2022 02:12:31 - INFO - __main__ - Sample 41905 of the training set: {'text': 'E3，2009开馆前一天外景内景照片欣赏', 'label': 4, 'input_ids': [101, 5907, 5661, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "05/21/2022 02:12:31 - INFO - __main__ - Sample 7296 of the training set: {'text': '海外求职：从推销员到巴克莱分行经理', 'label': 0, 'input_ids': [101, 100, 5663, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "05/21/2022 02:12:31 - INFO - __main__ - Sample 1639 of the training set: {'text': '中国向索马里提供100万美元无偿援助', 'label': 5, 'input_ids': [101, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "05/21/2022 02:12:32 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.2.2/metrics/accuracy/accuracy.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpq3upllu_\n",
            "Downloading builder script: 4.21kB [00:00, 5.20MB/s]       \n",
            "05/21/2022 02:12:32 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.2.2/metrics/accuracy/accuracy.py in cache at /root/.cache/huggingface/datasets/downloads/302a01b7f8daba5e0dc4e13a39ce01c6bc75cb5206e425cb0136bceff99dda46.32b3507481ea2e26fd6a2b34c9976e9da377302faaf35089eb1cd971d41bb0ff.py\n",
            "05/21/2022 02:12:32 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/302a01b7f8daba5e0dc4e13a39ce01c6bc75cb5206e425cb0136bceff99dda46.32b3507481ea2e26fd6a2b34c9976e9da377302faaf35089eb1cd971d41bb0ff.py\n",
            "[INFO|trainer.py:502] 2022-05-21 02:12:46,440 >> Using amp half precision backend\n",
            "[INFO|trainer.py:623] 2022-05-21 02:12:46,441 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1419] 2022-05-21 02:12:46,452 >> ***** Running training *****\n",
            "[INFO|trainer.py:1420] 2022-05-21 02:12:46,452 >>   Num examples = 50000\n",
            "[INFO|trainer.py:1421] 2022-05-21 02:12:46,452 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1422] 2022-05-21 02:12:46,452 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1423] 2022-05-21 02:12:46,452 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1424] 2022-05-21 02:12:46,452 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1425] 2022-05-21 02:12:46,452 >>   Total optimization steps = 9375\n",
            "{'loss': 1.6705, 'learning_rate': 1.893546666666667e-05, 'epoch': 0.16}\n",
            "{'loss': 1.554, 'learning_rate': 1.78688e-05, 'epoch': 0.32}\n",
            "{'loss': 1.4811, 'learning_rate': 1.6802133333333336e-05, 'epoch': 0.48}\n",
            "{'loss': 1.4995, 'learning_rate': 1.573546666666667e-05, 'epoch': 0.64}\n",
            "{'loss': 1.4818, 'learning_rate': 1.4670933333333336e-05, 'epoch': 0.8}\n",
            "{'loss': 1.4716, 'learning_rate': 1.3604266666666668e-05, 'epoch': 0.96}\n",
            " 33% 3125/9375 [08:03<16:18,  6.39it/s][INFO|trainer.py:2340] 2022-05-21 02:20:49,554 >> Saving model checkpoint to /content/woberta/checkpoint-3125\n",
            "[INFO|configuration_utils.py:446] 2022-05-21 02:20:49,555 >> Configuration saved in /content/woberta/checkpoint-3125/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-21 02:20:51,035 >> Model weights saved in /content/woberta/checkpoint-3125/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-05-21 02:20:51,036 >> tokenizer config file saved in /content/woberta/checkpoint-3125/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-05-21 02:20:51,036 >> Special tokens file saved in /content/woberta/checkpoint-3125/special_tokens_map.json\n",
            "{'loss': 1.4429, 'learning_rate': 1.25376e-05, 'epoch': 1.12}\n",
            "{'loss': 1.4454, 'learning_rate': 1.1470933333333334e-05, 'epoch': 1.28}\n",
            "{'loss': 1.4123, 'learning_rate': 1.0406400000000001e-05, 'epoch': 1.44}\n",
            "{'loss': 1.4317, 'learning_rate': 9.339733333333335e-06, 'epoch': 1.6}\n",
            "{'loss': 1.4454, 'learning_rate': 8.273066666666667e-06, 'epoch': 1.76}\n",
            "{'loss': 1.4312, 'learning_rate': 7.2064e-06, 'epoch': 1.92}\n",
            " 67% 6250/9375 [16:15<08:06,  6.43it/s][INFO|trainer.py:2340] 2022-05-21 02:29:02,134 >> Saving model checkpoint to /content/woberta/checkpoint-6250\n",
            "[INFO|configuration_utils.py:446] 2022-05-21 02:29:02,135 >> Configuration saved in /content/woberta/checkpoint-6250/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-21 02:29:03,665 >> Model weights saved in /content/woberta/checkpoint-6250/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-05-21 02:29:03,666 >> tokenizer config file saved in /content/woberta/checkpoint-6250/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-05-21 02:29:03,666 >> Special tokens file saved in /content/woberta/checkpoint-6250/special_tokens_map.json\n",
            "{'loss': 1.417, 'learning_rate': 6.139733333333334e-06, 'epoch': 2.08}\n",
            "{'loss': 1.4264, 'learning_rate': 5.073066666666667e-06, 'epoch': 2.24}\n",
            "{'loss': 1.3897, 'learning_rate': 4.0064e-06, 'epoch': 2.4}\n",
            "{'loss': 1.4016, 'learning_rate': 2.9397333333333334e-06, 'epoch': 2.56}\n",
            "{'loss': 1.3915, 'learning_rate': 1.873066666666667e-06, 'epoch': 2.72}\n",
            "{'loss': 1.3997, 'learning_rate': 8.085333333333334e-07, 'epoch': 2.88}\n",
            "100% 9375/9375 [24:28<00:00,  6.40it/s][INFO|trainer.py:2340] 2022-05-21 02:37:14,859 >> Saving model checkpoint to /content/woberta/checkpoint-9375\n",
            "[INFO|configuration_utils.py:446] 2022-05-21 02:37:14,860 >> Configuration saved in /content/woberta/checkpoint-9375/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-21 02:37:16,381 >> Model weights saved in /content/woberta/checkpoint-9375/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-05-21 02:37:16,382 >> tokenizer config file saved in /content/woberta/checkpoint-9375/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-05-21 02:37:16,382 >> Special tokens file saved in /content/woberta/checkpoint-9375/special_tokens_map.json\n",
            "[INFO|trainer.py:1662] 2022-05-21 02:37:20,724 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1474.272, 'train_samples_per_second': 101.745, 'train_steps_per_second': 6.359, 'train_loss': 1.4529182747395832, 'epoch': 3.0}\n",
            "100% 9375/9375 [24:34<00:00,  6.36it/s]\n",
            "[INFO|trainer.py:2340] 2022-05-21 02:37:20,739 >> Saving model checkpoint to /content/woberta/\n",
            "[INFO|configuration_utils.py:446] 2022-05-21 02:37:20,740 >> Configuration saved in /content/woberta/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-21 02:37:22,593 >> Model weights saved in /content/woberta/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-05-21 02:37:22,594 >> tokenizer config file saved in /content/woberta/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-05-21 02:37:22,594 >> Special tokens file saved in /content/woberta/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     1.4529\n",
            "  train_runtime            = 0:24:34.27\n",
            "  train_samples            =      50000\n",
            "  train_samples_per_second =    101.745\n",
            "  train_steps_per_second   =      6.359\n",
            "05/21/2022 02:37:22 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:623] 2022-05-21 02:37:22,678 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-05-21 02:37:22,681 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-05-21 02:37:22,681 >>   Num examples = 5000\n",
            "[INFO|trainer.py:2595] 2022-05-21 02:37:22,681 >>   Batch size = 8\n",
            "100% 625/625 [00:13<00:00, 44.94it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.4742\n",
            "  eval_loss               =      1.428\n",
            "  eval_runtime            = 0:00:14.04\n",
            "  eval_samples            =       5000\n",
            "  eval_samples_per_second =    355.917\n",
            "  eval_steps_per_second   =      44.49\n",
            "[INFO|modelcard.py:460] 2022-05-21 02:37:37,673 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.4742000102996826}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###这wobert\n",
        "跑了3轮，你准确率0.47？，肯定是huggingface上模型是broken的"
      ],
      "metadata": {
        "id": "1xvDm7GCfMD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#再试试mac-bert，看在huggingface上用的挺多的\n",
        "!python transformers/examples/pytorch/text-classification/run_glue.py \\\n",
        "  --model_name_or_path hfl/chinese-macbert-base \\\n",
        "  --train_file train.csv \\\n",
        "  --validation_file valid.csv\\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 16 \\\n",
        "  --fp16 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --save_strategy epoch \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --output_dir /content/macbert/"
      ],
      "metadata": {
        "id": "Hi75hzcqbkZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MacBert acc 0.963\n",
        "挺好的了，看来macbert将mask替换为近义词，来解决pretrain和fine-tune不一致问题，效果还不凑合吧。"
      ],
      "metadata": {
        "id": "gOQvj3GRo7VH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#再Roberta，精调的bert，感觉应该算是效果应该还行吧\n",
        "\n",
        "跑了3轮 0.9638，可能模型大一些，需要跑的step多一些可能会更好。\n"
      ],
      "metadata": {
        "id": "CYcYbX1bpWHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python transformers/examples/pytorch/text-classification/run_glue.py \\\n",
        "  --model_name_or_path hfl/chinese-roberta-wwm-ext \\\n",
        "  --train_file train.csv \\\n",
        "  --validation_file valid.csv\\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 16 \\\n",
        "  --fp16 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --save_strategy epoch \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --output_dir /content/roberta/"
      ],
      "metadata": {
        "id": "DoujLIvEpn_z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}